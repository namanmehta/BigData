from pyspark.mllib.clustering import KMeans, KMeansModel
from numpy import array
from math import sqrt
from pyspark import SparkConf, SparkContext
conf = SparkConf().setMaster("local").setAppName("friends-by-age")
sc = SparkContext.getOrCreate(conf = conf)
sc
sc.version

#Load and Parse the data
data = sc.textFile("/FileStore/tables/xAPI_Edu_Data-2676d.csv")
parsedData = data.map(lambda line: array([x for x in line.split(',')]))
parsedData.take(5)


pd=parsedData.zipWithIndex().filter(lambda tup: tup[1] > 0).map(lambda tup: tup[0])  #removing first row


#question 1
clusterParameter = pd.map(lambda x : array([float(x[9]),float(x[10]),float(x[11]),float(x[12])]))

clusters = KMeans.train(clusterParameter, 4, maxIterations=10, initializationMode="random")

# Evaluate clustering by computing Within Set Sum of Squared Errors
def error(point):
  center = clusters.centers[clusters.predict(point)]
  return sqrt(sum([x**2 for x in (point - center)]))
#WSSSE = clusterParameter.map(lambda point: error(point)).reduce(lambda x, y: x + y)
#print("Within Set Sum of Squared Error = " + str(WSSSE))

k=1
WSS={}
while(k<6):
  clusters = KMeans.train(clusterParameter, k=k, maxIterations=10, initializationMode="random")
  WSSE = clusterParameter.map(lambda point: error(point)).reduce(lambda x, y: x + y)
  WSS[k]=WSSE
  k=k+1
cluster_assignments = clusters.predict(clusterParameter)
assigned_clusters = clusterParameter.zip(cluster_assignments)
centroids = clusters.clusterCenters
clusterone = assigned_clusters.map(lambda x : (x,1))
clustercount = clusterone.map(lambda x : (x[0][1],x[1])).reduceByKey(lambda x,y : x+y)
clustercount.collect()


#question2

cp=pd.map(lambda x: (float(x[9])+float(x[10])+float(x[11])+float(x[12]))/4)

for i in cp.collect():
  print(i)
